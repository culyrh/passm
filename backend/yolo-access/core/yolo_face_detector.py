# YOLO + face_recognition í•˜ì´ë¸Œë¦¬ë“œ ì–¼êµ´ ì¸ì‹ ì‹œìŠ¤í…œ

import cv2
import torch
import numpy as np
from ultralytics import YOLO
import face_recognition
from pathlib import Path
import time
import requests
from typing import List, Dict, Any, Optional, Tuple
import logging
from PIL import Image
import os
import pickle

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class YOLOFaceDetector:
    """YOLO (íƒì§€) + face_recognition (ì¸ì‹) í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ"""
    
    def __init__(self, model_type: str = "yolov8n"):
        """
        í•˜ì´ë¸Œë¦¬ë“œ ì–¼êµ´ ì¸ì‹ê¸° ì´ˆê¸°í™”
        
        Args:
            model_type: YOLO ëª¨ë¸ íƒ€ìž…
        """
        self.model_type = model_type
        self.yolo_model = None
        self.device = 'cpu'
        self.confidence_threshold = 0.5
        self.iou_threshold = 0.45
        
        # YOLO ëª¨ë¸ ë¡œë“œ
        self.load_yolo_model()
        
        # face_recognitionìš© ë°ì´í„°
        self.known_face_encodings = []
        self.known_face_names = []
        self.user_face_data = {}  # ê° ì‚¬ìš©ìžë³„ ì—¬ëŸ¬ ìž„ë² ë”© ì €ìž¥
        
        logger.info(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ì–¼êµ´ ì¸ì‹ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_yolo_model(self):
        """YOLO ëª¨ë¸ ë¡œë“œ (ì–¼êµ´ íƒì§€ìš©)"""
        try:
            logger.info(f"ðŸ”„ YOLO {self.model_type} ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.yolo_model = YOLO("yolov8n-face.pt")
            self.yolo_model.to(self.device)
            
            # ì›Œë°ì—…
            dummy_image = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)
            _ = self.yolo_model(dummy_image, verbose=False)
            
            logger.info(f"âœ… YOLO ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ YOLO ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.yolo_model = None
    
    def detect_faces_with_yolo(self, image: np.ndarray) -> Tuple[List[Dict], float]:
        """
        YOLOë¡œ ì–¼êµ´ íƒì§€ (ë¹ ë¥´ê³  ì •í™•í•œ ìœ„ì¹˜ ì°¾ê¸°)
        
        Returns:
            (ì–¼êµ´ ìœ„ì¹˜ ë¦¬ìŠ¤íŠ¸, ì²˜ë¦¬ ì‹œê°„)
        """
        if self.yolo_model is None:
            # YOLO ì‹¤íŒ¨ì‹œ face_recognitionìœ¼ë¡œ í´ë°±
            return self.detect_faces_with_face_recognition(image)
        
        start_time = time.time()
        
        try:
            # YOLO ì¶”ë¡ 
            results = self.yolo_model(
                image,
                conf=self.confidence_threshold,
                iou=self.iou_threshold,
                verbose=False,
                device=self.device
            )
            
            faces = []
            
            if results and len(results) > 0:
                result = results[0]
                
                if result.boxes is not None:
                    boxes = result.boxes
                    
                    for box in boxes:
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        confidence = float(box.conf[0].cpu().numpy())
                        class_id = int(box.cls[0].cpu().numpy())
                        class_name = self.yolo_model.names.get(class_id, "unknown")
                        
                        # ì‚¬ëžŒ í´ëž˜ìŠ¤ë§Œ í•„í„°ë§
                        if class_name.lower() in ['person', '0']:
                            # YOLOëŠ” ì „ì‹ ì„ íƒì§€í•˜ë¯€ë¡œ ì–¼êµ´ ì˜ì—­ ì¶”ì •
                            # ìƒì²´ ìƒë‹¨ 1/4 ì˜ì—­ì„ ì–¼êµ´ë¡œ ì¶”ì •
                            face_height = (y2 - y1) * 0.25
                            face_width = (x2 - x1) * 0.4
                            
                            # ì–¼êµ´ ì¤‘ì‹¬ì„ ìƒì²´ ìƒë‹¨ìœ¼ë¡œ ì¶”ì •
                            center_x = (x1 + x2) / 2
                            face_y1 = y1
                            face_y2 = y1 + face_height
                            face_x1 = center_x - face_width / 2
                            face_x2 = center_x + face_width / 2
                            
                            # ì´ë¯¸ì§€ ê²½ê³„ ì²´í¬
                            h, w = image.shape[:2]
                            face_x1 = max(0, int(face_x1))
                            face_y1 = max(0, int(face_y1))
                            face_x2 = min(w, int(face_x2))
                            face_y2 = min(h, int(face_y2))
                            
                            if face_x2 > face_x1 and face_y2 > face_y1:
                                face_info = {
                                    'bbox': [face_x1, face_y1, face_x2-face_x1, face_y2-face_y1],
                                    'confidence': float(confidence),
                                    'method': 'yolo',
                                    'area': int((face_x2-face_x1) * (face_y2-face_y1))
                                }
                                faces.append(face_info)
            
            processing_time = time.time() - start_time
            
            # ë©´ì  ê¸°ì¤€ ì •ë ¬
            faces.sort(key=lambda x: x['area'], reverse=True)
            
            # YOLOë¡œ ì–¼êµ´ì„ ëª» ì°¾ìœ¼ë©´ face_recognitionìœ¼ë¡œ ìž¬ì‹œë„
            if not faces:
                logger.info("ðŸ”„ YOLO íƒì§€ ì‹¤íŒ¨, face_recognitionìœ¼ë¡œ ìž¬ì‹œë„...")
                return self.detect_faces_with_face_recognition(image)
            
            return faces, float(processing_time)
            
        except Exception as e:
            logger.error(f"âŒ YOLO ì–¼êµ´ íƒì§€ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ì‹œ face_recognitionìœ¼ë¡œ í´ë°±
            return self.detect_faces_with_face_recognition(image)
    
    def detect_faces_with_face_recognition(self, image: np.ndarray) -> Tuple[List[Dict], float]:
        """
        face_recognitionìœ¼ë¡œ ì–¼êµ´ íƒì§€ (YOLO ì‹¤íŒ¨ì‹œ ë°±ì—…ìš©)
        """
        start_time = time.time()
        
        try:
            # RGB ë³€í™˜ (face_recognitionì€ RGB ì‚¬ìš©)
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # ì–¼êµ´ ìœ„ì¹˜ íƒì§€
            face_locations = face_recognition.face_locations(rgb_image, model="hog")  # CNNì€ ëŠë¦¼
            
            faces = []
            for i, (top, right, bottom, left) in enumerate(face_locations):
                face_info = {
                    'bbox': [left, top, right-left, bottom-top],
                    'confidence': 0.9,  # face_recognitionì€ confidence ì—†ìŒ
                    'method': 'face_recognition',
                    'area': (right-left) * (bottom-top)
                }
                faces.append(face_info)
            
            processing_time = time.time() - start_time
            faces.sort(key=lambda x: x['area'], reverse=True)
            
            return faces, float(processing_time)
            
        except Exception as e:
            logger.error(f"âŒ face_recognition íƒì§€ ì˜¤ë¥˜: {e}")
            return [], float(time.time() - start_time)
    
    def extract_face_encoding(self, image: np.ndarray, face_bbox: List[int]) -> np.ndarray:
        """
        face_recognitionìœ¼ë¡œ 128ì°¨ì› ì–¼êµ´ ìž„ë² ë”© ì¶”ì¶œ (ê³ ì •ë°€)
        """
        try:
            x, y, w, h = face_bbox
            
            # ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ
            face_region = image[y:y+h, x:x+w]
            if face_region.size == 0:
                return np.array([])
            
            # RGB ë³€í™˜
            rgb_face = cv2.cvtColor(face_region, cv2.COLOR_BGR2RGB)
            
            # face_recognitionìœ¼ë¡œ ìž„ë² ë”© ì¶”ì¶œ
            encodings = face_recognition.face_encodings(rgb_face, model="large")  # ì •í™•ë„ ìš°ì„ 
            
            if encodings:
                return encodings[0]  # 128ì°¨ì› ë²¡í„°
            else:
                return np.array([])
                
        except Exception as e:
            logger.error(f"âŒ ì–¼êµ´ ì¸ì½”ë”© ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return np.array([])
    
    def load_user_database(self, users_dir: str = "data/users"):
        """ì‚¬ìš©ìž ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ (face_recognition ë°©ì‹)"""
        try:
            if not os.path.exists(users_dir):
                logger.warning(f"ì‚¬ìš©ìž ë°ì´í„° í´ë” ì—†ìŒ: {users_dir}")
                return
            
            self.known_face_encodings = []
            self.known_face_names = []
            self.user_face_data = {}
            
            for user_name in os.listdir(users_dir):
                user_path = os.path.join(users_dir, user_name)
                
                if os.path.isdir(user_path):
                    logger.info(f"ðŸ”„ {user_name} ë°ì´í„° ë¡œë“œ ì¤‘...")
                    
                    user_encodings = []
                    processed_count = 0
                    
                    for image_file in os.listdir(user_path):
                        if image_file.lower().endswith(('.jpg', '.jpeg', '.png')):
                            image_path = os.path.join(user_path, image_file)
                            
                            try:
                                # ì´ë¯¸ì§€ ë¡œë“œ
                                image = cv2.imread(image_path)
                                if image is None:
                                    continue
                                
                                # YOLOë¡œ ì–¼êµ´ íƒì§€
                                faces, _ = self.detect_faces_with_yolo(image)
                                
                                if faces:
                                    # ê°€ìž¥ í° ì–¼êµ´ ì„ íƒ
                                    largest_face = max(faces, key=lambda x: x['area'])
                                    
                                    # face_recognitionìœ¼ë¡œ ì¸ì½”ë”© ì¶”ì¶œ
                                    encoding = self.extract_face_encoding(image, largest_face['bbox'])
                                    
                                    if len(encoding) > 0:
                                        user_encodings.append(encoding)
                                        processed_count += 1
                                        
                            except Exception as e:
                                logger.warning(f"âš ï¸ {image_file} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                                continue
                    
                    if user_encodings:
                        # ê° ì‚¬ì§„ì˜ ì¸ì½”ë”©ì„ ê°œë³„ ì €ìž¥ (í‰ê· ë‚´ì§€ ì•ŠìŒ)
                        self.user_face_data[user_name] = user_encodings
                        
                        # ëŒ€í‘œ ì¸ì½”ë”© (ì²« ë²ˆì§¸) ì €ìž¥
                        self.known_face_encodings.append(user_encodings[0])
                        self.known_face_names.append(user_name)
                        
                        logger.info(f"âœ… {user_name}: {processed_count}ìž¥ ì²˜ë¦¬ ì™„ë£Œ")
                    else:
                        logger.warning(f"âš ï¸ {user_name}: ì²˜ë¦¬ ê°€ëŠ¥í•œ ì´ë¯¸ì§€ ì—†ìŒ")
            
            logger.info(f"ðŸŽ¯ ì´ {len(self.known_face_names)}ëª…ì˜ ì‚¬ìš©ìž ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ì‚¬ìš©ìž ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def recognize_user(self, face_encoding: np.ndarray, tolerance: float = 0.6) -> Dict:
        """
        face_recognitionìœ¼ë¡œ ì‚¬ìš©ìž ì¸ì‹ (ê³ ì •ë°€)
        """
        if len(self.known_face_encodings) == 0:
            return {
                'recognized': False,
                'user_name': 'ë°ì´í„°ë² ì´ìŠ¤ ì—†ìŒ',
                'confidence': 0.0,
                'message': 'ë“±ë¡ëœ ì‚¬ìš©ìž ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'
            }
        
        if len(face_encoding) == 0:
            return {
                'recognized': False,
                'user_name': 'íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨',
                'confidence': 0.0,
                'message': 'ì–¼êµ´ íŠ¹ì§•ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'
            }
        
        try:
            # face_recognitionì˜ compare_faces ì‚¬ìš©
            matches = face_recognition.compare_faces(
                self.known_face_encodings, 
                face_encoding, 
                tolerance=tolerance
            )
            
            # ê±°ë¦¬ ê³„ì‚°
            distances = face_recognition.face_distance(self.known_face_encodings, face_encoding)
            
            if len(distances) > 0:
                best_match_index = np.argmin(distances)
                best_distance = distances[best_match_index]
                confidence = round((1 - best_distance) * 100, 2)
                
                # ë§¤ì¹­ ì„±ê³µì‹œ
                if matches[best_match_index] and best_distance < tolerance:
                    user_name = self.known_face_names[best_match_index]
                    
                    # í•´ë‹¹ ì‚¬ìš©ìžì˜ ëª¨ë“  ì‚¬ì§„ê³¼ ì¶”ê°€ ë¹„êµ (ì •í™•ë„ í–¥ìƒ)
                    if user_name in self.user_face_data:
                        user_distances = face_recognition.face_distance(
                            self.user_face_data[user_name], 
                            face_encoding
                        )
                        avg_distance = np.mean(user_distances)
                        final_confidence = round((1 - avg_distance) * 100, 2)
                    else:
                        final_confidence = confidence
                    
                    return {
                        'recognized': True,
                        'user_name': str(user_name),
                        'confidence': float(final_confidence),
                        'distance': float(best_distance),
                        'tolerance': float(tolerance),
                        'message': f'{user_name} ì¸ì‹ ì„±ê³µ'
                    }
                else:
                    return {
                        'recognized': False,
                        'user_name': 'ë¯¸ë“±ë¡ ì¸ë¬¼',
                        'confidence': float(confidence),
                        'distance': float(best_distance),
                        'tolerance': float(tolerance),
                        'best_match': str(self.known_face_names[best_match_index]),
                        'message': f'ê±°ë¦¬ {round(best_distance, 3)} > í—ˆìš©ê°’ {tolerance}'
                    }
            else:
                return {
                    'recognized': False,
                    'user_name': 'ë¹„êµ ì‹¤íŒ¨',
                    'confidence': 0.0,
                    'message': 'ê±°ë¦¬ ê³„ì‚° ì‹¤íŒ¨'
                }
                
        except Exception as e:
            logger.error(f"âŒ ì‚¬ìš©ìž ì¸ì‹ ì˜¤ë¥˜: {e}")
            return {
                'recognized': False,
                'user_name': 'ì¸ì‹ ì˜¤ë¥˜',
                'confidence': 0.0,
                'message': f'ì¸ì‹ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}'
            }
    
    def process_image(self, image: np.ndarray) -> Dict:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ì´ë¯¸ì§€ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        1. YOLOë¡œ ë¹ ë¥¸ ì–¼êµ´ íƒì§€
        2. face_recognitionìœ¼ë¡œ ì •í™•í•œ ì¸ì‹
        """
        start_time = time.time()
        
        try:
            # 1. YOLOë¡œ ì–¼êµ´ íƒì§€
            faces, detection_time = self.detect_faces_with_yolo(image)
            
            if not faces:
                return {
                    'success': False,
                    'faces_found': 0,
                    'message': 'ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤',
                    'detection_time': float(round(detection_time, 3)),
                    'total_time': float(round(time.time() - start_time, 3))
                }
            
            # 2. ê°€ìž¥ í° ì–¼êµ´ ì„ íƒ
            main_face = faces[0]
            
            # 3. face_recognitionìœ¼ë¡œ ì¸ì½”ë”© ì¶”ì¶œ
            encoding_start = time.time()
            face_encoding = self.extract_face_encoding(image, main_face['bbox'])
            encoding_time = time.time() - encoding_start
            
            # 4. ì‚¬ìš©ìž ì¸ì‹
            recognition_start = time.time()
            recognition_result = self.recognize_user(face_encoding)
            recognition_time = time.time() - recognition_start
            
            total_time = time.time() - start_time
            
            # ê²°ê³¼ í†µí•©
            result = {
                'success': True,
                'faces_found': int(len(faces)),
                'main_face': {
                    **main_face,
                    **recognition_result
                },
                'all_faces': faces,
                'performance': {
                    'detection_time': float(round(detection_time, 3)),
                    'encoding_time': float(round(encoding_time, 3)),
                    'recognition_time': float(round(recognition_time, 3)),
                    'total_time': float(round(total_time, 3))
                },
                'method': 'hybrid_yolo_face_recognition'
            }
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return {
                'success': False,
                'faces_found': 0,
                'message': f'ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}',
                'total_time': float(round(time.time() - start_time, 3))
            }
    
    def get_model_info(self) -> Dict:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜ (ê¸°ì¡´ main.pyì™€ í˜¸í™˜)"""
        return {
            # ê¸°ì¡´ í‚¤ë“¤ (main.py í˜¸í™˜)
            'model_type': self.model_type,
            'device': self.device,
            'confidence_threshold': self.confidence_threshold,
            'iou_threshold': self.iou_threshold,
            'registered_users_count': len(self.known_face_names),
            'registered_users': self.known_face_names,
            'model_loaded': self.yolo_model is not None,
            
            # ìƒˆë¡œìš´ í•˜ì´ë¸Œë¦¬ë“œ ì •ë³´
            'detection_model': self.model_type,
            'recognition_model': 'face_recognition (dlib)',
            'face_recognition_tolerance': 0.6,
            'yolo_loaded': self.yolo_model is not None,
            'face_recognition_loaded': len(self.known_face_encodings) > 0,
            'system_type': 'hybrid_yolo_face_recognition'
        }